import textstat
import json
import sys
from transformers import logging
logging.set_verbosity_error()
import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# Copy from modeling.utils
from typing import Dict, List
from rouge_score import rouge_scorer, scoring
import re
from filelock import FileLock
try:
    import nltk
    NLTK_AVAILABLE = True
except (ImportError, ModuleNotFoundError):
    NLTK_AVAILABLE = False
if NLTK_AVAILABLE:
    with FileLock(".lock") as lock:
        nltk.download("punkt", quiet=True)

def add_newline_to_end_of_each_sentence(x: str) -> str:
    """This was added to get rougeLsum scores matching published rougeL scores for BART and PEGASUS."""
    re.sub("<n>", "", x)  # remove pegasus newline char
    assert NLTK_AVAILABLE, "nltk must be installed to separate newlines between sentences. (pip install nltk)"
    return "\n".join(nltk.sent_tokenize(x))

ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
def calculate_rouge(
    pred_lns: List[str],
    tgt_lns: List[str],
    use_stemmer=True,
    rouge_keys=ROUGE_KEYS,
    return_precision_and_recall=False,
    bootstrap_aggregation=True,
    newline_sep=True,
) -> Dict:
    """Calculate rouge using rouge_scorer package.

    Args:
        pred_lns: list of summaries generated by model
        tgt_lns: list of groundtruth summaries (e.g. contents of val.target)
        use_stemmer:  Bool indicating whether Porter stemmer should be used to
        strip word suffixes to improve matching.
        rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
        return_precision_and_recall: (False) whether to also return precision and recall.
        bootstrap_aggregation: whether to do the typical bootstrap resampling of scores. Defaults to True, if False
            this function returns a collections.defaultdict[metric: list of values for each observation for each subscore]``
        newline_sep:(default=True) whether to add newline between sentences. This is essential for calculation rougeL
        on multi sentence summaries (CNN/DM dataset).

    Returns:
         Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys

    """
    scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)
    aggregator = scoring.BootstrapAggregator()
    for pred, tgt in zip(tgt_lns, pred_lns):
        # rougeLsum expects "\n" separated sentences within a summary
        if newline_sep:
            pred = add_newline_to_end_of_each_sentence(pred)
            tgt = add_newline_to_end_of_each_sentence(tgt)
        scores = scorer.score(pred, tgt)
        aggregator.add_scores(scores)

    if bootstrap_aggregation:
        result = aggregator.aggregate()
        if return_precision_and_recall:
            return extract_rouge_mid_statistics(result)  # here we return dict
        else:
            return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}

    else:
        return aggregator._scores  # here we return defaultdict(list)

KEY = 'gen'
data = json.load(open(sys.argv[1], "r"))

# Readability: NLTK
import nltk
import collections
import re
VC = re.compile('[aeiou]+[^aeiou]+', re.I)
def count_syllables(word):
    return len(VC.findall(word))
def fk_score_nltk(test_data):
    sents = nltk.sent_tokenize(test_data)
    words = nltk.word_tokenize(test_data)
    num_sents = len(sents)
    num_words = len(words)
    num_syllables = sum(count_syllables(w) for w in words)
    score = 0.39 * (float(num_words) / num_sents) + 11.8 * (num_syllables / float(num_words)) - 15.59
    return score
def ari_score_nltk(test_data):
    sents = nltk.sent_tokenize(test_data)
    words = nltk.word_tokenize(test_data)
    num_sents = len(sents)
    num_words = len(words)
    num_chars = sum(len(w) for w in words)
    score = 4.71 * (num_chars / float(num_words)) + 0.5 * (float(num_words) / num_sents) - 21.43
    return score
fk_scores = []
ari_scores = []
for record in data:
    text = record[KEY]
    fk_scores.append(fk_score_nltk(text))
    ari_scores.append(ari_score_nltk(text))
print("FK and ARI scores", sum(fk_scores) / len(fk_scores), sum(ari_scores) / len(ari_scores))

# Rouge: from base codes
sys_sents = [record['gen'] for record in data]
refs_sents = [record['pls'] for record in data]
scores = calculate_rouge(sys_sents, refs_sents)
print('R-1 = {:.2f}'.format(scores['rouge1']))
print('R-2 = {:.2f}'.format(scores['rouge2']))
print('R-L = {:.2f}'.format(scores['rougeLsum']))

# BLEU: Huggingface's evaluate
from evaluate import load
bleu_scores = []
bleu = load("bleu")
for record in data:
    pls = record["pls"]
    gen = record["gen"]
    predictions = [gen]
    references = [[pls]]
    results = bleu.compute(predictions=predictions, references=references)
    bleu_scores.append(results["bleu"])
print("evaluate bleu", sum(bleu_scores) / len(bleu_scores))

# SARI: Huggingface's evaluate
sari_scores = []
sari = load("sari")
for record in data:
    abstract = record["abstract"]
    pls = record["pls"]
    gen = record["gen"]
    sources=[abstract]
    predictions=[gen]
    references=[[pls]]
    sari_score = sari.compute(sources=sources, predictions=predictions, references=references)
    sari_scores.append(sari_score["sari"])
print("sari", sum(sari_scores) / len(sari_scores))

# BLEURT
from bleurt import score
scorer = score.BleurtScorer()
bleurt = []
for record in data:
    pls = record["pls"]
    gen = record["gen"]
    candidates = [gen]
    references = [pls]
    scores = scorer.score(references=references, candidates=candidates)
    bleurt.append(scores[0])
print("bleurt", sum(bleurt) / len(bleurt))

# Bertscore
# from bert_score import score
# bertscore = []
# for record in data:
#     pls = record["pls"]
#     gen = record["gen"]
#     candidates = [gen]
#     references = [pls]
#     bertscore.append(score(candidates, [references], lang="en", rescale_with_baseline=True)[2].mean().item())
# print("bertscore", sum(bertscore) / len(bertscore))
